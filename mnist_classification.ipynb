{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ <b><u>Exercise objectives</u></b>\n",
    "- Understand the *MNIST* dataset \n",
    "- Design your first **Convolutional Neural Network** (*CNN*) and answer questions such as:\n",
    "    - what are *Convolutional Layers*? \n",
    "    - how many *parameters* are involved in such a layer?\n",
    "- Train this CNN on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöÄ <b><u>Let's get started!</u></b>\n",
    "\n",
    "Imagine that we are  back in time into the 90's.\n",
    "You work at a *Post Office* and you have to deal with an enormous amount of letters on a daily basis. How could you automate the process of reading the ZIP Codes, which are a combination of 5 handwritten digits? \n",
    "\n",
    "This task, called the **Handwriting Recognition**, used to be a very complex problem back in those days. It was solved by *Bell Labs* (among others) where one of the Deep Learning gurus, [*Yann Le Cun*](https://en.wikipedia.org/wiki/Yann_LeCun), used to work.\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Handwriting_recognition):\n",
    "\n",
    "> Handwriting recognition (HWR), also known as Handwritten Text Recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Number recognition](recognition.gif)\n",
    "\n",
    "*Note: The animation above is just here to help you visualize what happens with the different images: <br/> $\\rightarrow$ For each image, once the CNN is trained, it will predict what digit is written. The inputs are the different digits and not one animation/video!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ü§î <b><u>How does this CNN work ?</u></b>\n",
    "\n",
    "- *Inputs*: Images (_each image shows a handwritten digit_)\n",
    "- *Target*: For each image, you want your CNN model to predict the correct digit (between 0 and 9)\n",
    "    - It is a **multi-class classification** task (more precisely a 10-class classification task since there are 10 different digits).\n",
    "\n",
    "üî¢ To improve the capacity of the Convolutional Neural Network to read these numbers, we need to feed it with many images representing handwritten digits. This is why the üìö [**MNIST dataset**](http://yann.lecun.com/exdb/mnist/) *(Mixed National Institute of Standards and Technology)* was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) The `MNIST` Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö Tensorflow/Keras offers multiple [**datasets**](https://www.tensorflow.org/api_docs/python/tf/keras/datasets) to play with:\n",
    "- *Vectors*: `boston_housing` (regression)\n",
    "- *Images* : `mnist`, `fashion_mnist`, `cifar10`, `cifar100` (classification)\n",
    "- *Texts*: `imbd`, `reuters` (classification/sentiment analysis)\n",
    "\n",
    "\n",
    "üíæ You can **load the MNIST dataset** with the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import datasets\n",
    "\n",
    "\n",
    "# Loading the MNIST Dataset...\n",
    "(X_train, y_train), (X_test, y_test) = datasets.mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "# The train set contains 60 000 images, each of them of size 28x28\n",
    "# The test set contains 10 000 images, each of them of size 28x28\n",
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.1) Exploring the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Let's have look at some handwritten digits of this MNIST dataset.** ‚ùì\n",
    "\n",
    "üñ® Print some images from the *train set*.\n",
    "\n",
    "<details>\n",
    "    <summary><i>Hints</i></summary>\n",
    "\n",
    "üí°*Hint*: use the `imshow` function from `matplotlib` with `cmap = \"gray\"`\n",
    "\n",
    "ü§® Note: if you don't specify this *cmap* argument, the weirdly displayed colors are just Matplotlib defaults...\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2c8ae5870>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[0], cmap=\"gray\")\n",
    "plt.imshow(X_train[4], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.2) Image Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùóÔ∏è **Neural Networks converge faster when the input data is somehow normalized** ‚ùóÔ∏è\n",
    "\n",
    "üë©üèª‚Äçüè´ How do we proceed for Convolutional Neural Networks ?\n",
    "* The `RBG` intensities are coded between 0 and 255. \n",
    "* We can simply divide the input data by the maximal value 255 to have all the pixels' intensities between 0 and 1 üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question ‚ùì As a first preprocessing step, please normalize your data.** \n",
    "\n",
    "Don't forget to do it both on your train data and your test data.\n",
    "\n",
    "(*Note: you can also center your data, by subtracting 0.5 from all the values, but it is not mandatory*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-02-21 07:25:16.795791: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Normalization \n",
    "\n",
    "normalizer = Normalization() # Instantiate a \"normalizer\" layer\n",
    "normalizer.adapt(X_train)\n",
    "normalizer.adapt(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.3) Inputs' dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÜ Remember that you have 60,000 training images and 10,000 test images, each of size $(28, 28)$. However...\n",
    "\n",
    "> ‚ùóÔ∏è  **`Convolutional Neural Network models need to be fed with images whose last dimension is the number of channels`.**  \n",
    "\n",
    "> üßëüèª‚Äçüè´ The shape of tensors fed into ***ConvNets*** is the following: `(NUMBER_OF_IMAGES, HEIGHT, WIDTH, CHANNELS)`\n",
    "\n",
    "üïµüèªThis last dimension is clearly missing here. Can you guess the reason why?\n",
    "<br>\n",
    "<details>\n",
    "    <summary><i>Answer<i></summary>\n",
    "        \n",
    "* All these $60000$ $ (28 \\times 28) $ pictures are black-and-white $ \\implies $ Each pixel lives on a spectrum from full black (0) to full white (1).\n",
    "        \n",
    "    * Theoretically, you don't need to know the number of channels for a black-and-white picture since there is only 1 channel (the \"whiteness\" of \"blackness\" of a pixel). However, it is still mandatory for the model to have this number of channels explicitly stated.\n",
    "        \n",
    "    * In comparison, colored pictures need multiple channels:\n",
    "        - the RGB system with 3 channels (<b><span style=\"color:red\">Red</span> <span style=\"color:green\">Green</span> <span style=\"color:blue\">Blue</span></b>)\n",
    "        - the CYMK system  with 4 channels (<b><span style=\"color:cyan\">Cyan</span> <span style=\"color:magenta\">Magenta</span> <span style=\"color:yellow\">Yellow</span> <span style=\"color:black\">Black</span></b>)\n",
    "        \n",
    "        \n",
    "</details>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: expanding dimensions** ‚ùì\n",
    "\n",
    "* Use the **`expand_dims`** to add one dimension at the end of the training data and test data.\n",
    "\n",
    "* Then, print the shapes of `X_train` and `X_test`. They should respectively be equal to $(60000, 28, 28, 1)$ and $(10000, 28, 28, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.backend import expand_dims\n",
    "\n",
    "X_train = np.expand_dims(X_train, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1.4) Target encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more thing to do for a multiclass classification task in Deep Leaning:\n",
    "\n",
    "üëâ _\"one-hot-encode\" the categories*_\n",
    "\n",
    "‚ùì **Question: encoding the labels** ‚ùì \n",
    "\n",
    "* Use **`to_categorical`** to transform your labels. \n",
    "* Store the results into two variables that you can call **`y_train_cat`** and **`y_test_cat`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "y_train_cat = to_categorical(y_train)\n",
    "y_test_cat = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check that you correctly used to_categorical\n",
    "assert(y_train_cat.shape == (60000,10))\n",
    "assert(y_test_cat.shape == (10000,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is now ready to be used. ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) The Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.1) Architecture and compilation of a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "‚ùì **Question: CNN Architecture and compilation** ‚ùì\n",
    "\n",
    "Now, let's build a <u>Convolutional Neural Network</u> that has: \n",
    "\n",
    "\n",
    "- a `Conv2D` layer with 8 filters, each of size $(4, 4)$, an input shape suitable for your task, the `relu` activation function, and `padding='same'`\n",
    "- a `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "- a second `Conv2D` layer with 16 filters, each of size $(3, 3)$, and the `relu` activation function\n",
    "- a second `MaxPool2D` layer with a `pool_size` equal to $(2, 2)$\n",
    "\n",
    "\n",
    "- a `Flatten` layer\n",
    "- a first `Dense` layer with 10 neurons and the `relu` activation function\n",
    "- a last (predictive) layer that is suited for your task\n",
    "\n",
    "In the function that initializes this model, do not forget to include the <u>compilation of the model</u>, which:\n",
    "* optimizes the `categorical_crossentropy` loss function,\n",
    "* with the `adam` optimizer, \n",
    "* and the `accuracy` as the metrics\n",
    "\n",
    "(*Note: you could add more classification metrics if you want but the dataset is well balanced!*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "\n",
    "def initialize_model():\n",
    "\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(8, (4,4), input_shape=(28, 28, 1), activation=\"relu\", padding='same'))\n",
    "    \n",
    "    ### Second Convolution & MaxPooling\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(layers.Conv2D(16, (3,3), input_shape=(28, 28, 1), activation=\"relu\"))\n",
    "    model.add(layers.MaxPool2D(pool_size=(2,2)))\n",
    "    ### Flattening\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    \n",
    "    ### One Fully Connected layer - \"Fully Connected\" is equivalent to saying \"Dense\"\n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    \n",
    "    ### Last layer - Classification Layer with 10 outputs corresponding to 10 digits\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "    ### Model compilation\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: number of trainable parameters in a convolutional layer** ‚ùì \n",
    "\n",
    "How many trainable parameters are there in your model?\n",
    "1. Compute them with ***model.summary( )*** first\n",
    "2. Recompute them manually to make sure you properly understood ***what influences the number of weights in a CNN***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 28, 28, 8)         136       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 14, 14, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 12, 12, 16)        1168      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 16)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 576)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                5770      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                110       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7,184\n",
      "Trainable params: 7,184\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.2) Training a CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: training a CNN** ‚ùì \n",
    "\n",
    "Initialize your model and fit it on the train data. \n",
    "- Do not forget to use a **Validation Set/Split** and an **Early Stopping criterion**. \n",
    "- Limit yourself to 5 epochs max in this challenge, just to save some precious time for the more advanced challenges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2625/2625 [==============================] - 6s 2ms/step - loss: 2.3127 - accuracy: 0.1246 - val_loss: 1.9168 - val_accuracy: 0.2744\n",
      "Epoch 2/5\n",
      "2625/2625 [==============================] - 5s 2ms/step - loss: 1.4073 - accuracy: 0.4603 - val_loss: 0.8041 - val_accuracy: 0.6682\n",
      "Epoch 3/5\n",
      "2625/2625 [==============================] - 5s 2ms/step - loss: 0.5383 - accuracy: 0.8078 - val_loss: 0.2942 - val_accuracy: 0.9225\n",
      "Epoch 4/5\n",
      "2625/2625 [==============================] - 5s 2ms/step - loss: 0.1884 - accuracy: 0.9543 - val_loss: 0.1538 - val_accuracy: 0.9609\n",
      "Epoch 5/5\n",
      "2625/2625 [==============================] - 5s 2ms/step - loss: 0.1260 - accuracy: 0.9664 - val_loss: 0.1332 - val_accuracy: 0.9632\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d1860850>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=20, restore_best_weights=True)\n",
    "\n",
    "model.fit(X_train, y_train_cat, \n",
    "          batch_size=16, \n",
    "          epochs=5, \n",
    "          validation_split=0.3,\n",
    "          callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: How many iterations does the CNN perform per epoch** ‚ùì\n",
    "\n",
    "_Note: it has nothing to do with the fact that this is a CNN. This is related to the concept of forward/backward propagation already covered during the previous lecture on optimizers, fitting, and losses üòâ_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "source": [
    "> YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Answer</i></summary>\n",
    "\n",
    "With `verbose = 1` when fitting your model, you have access to crucial information about your training procedure.\n",
    "    \n",
    "Remember that we've just trained our CNN model on $60000$ training images\n",
    "\n",
    "If the chosen batch size is 32: \n",
    "\n",
    "* For each epoch, we have $ \\large \\lceil \\frac{60000}{32} \\rceil = 1875$ minibatches <br/>\n",
    "* The _validation_split_ is equal to $0.3$ - which means that within one single epoch, there are:\n",
    "    * $ \\lceil 1875 \\times (1 - 0.3) \\rceil = \\lceil 1312.5 \\rceil = 1313$ batches are used to compute the `train_loss` \n",
    "    * $ 1875 - 1312 = 562 $ batches are used to compute the `val_loss`\n",
    "    * **The parameters are updated 1313 times per epoch** as there are 1313 forward/backward propagations per epoch !!!\n",
    "\n",
    "\n",
    "üëâ With so many updates of the weights within one epoch, you can understand why this CNN model converges even with a limited number of epochs.\n",
    "\n",
    "</details>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2.3) Evaluating its performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Evaluating your CNN** ‚ùì \n",
    "\n",
    "What is your **`accuracy on the test set?`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.968999981880188"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "accuracy = result[1]\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéâ You should already be impressed by your CNN skills! Reaching over 95% accuracy!\n",
    "\n",
    "üî• You solved what was a very hard problem 30 years ago with your own CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Congratulations!**\n",
    "\n",
    "üíæ Don't forget to `git add/commit/push` your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
